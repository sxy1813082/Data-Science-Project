% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the academic year, say 2013/14 say).
\documentclass[ % the name of the author
                    author={Xinyang Song},
                % the name of the supervisor
                supervisor={Dr. Edwin Simpson},
                % the degree programme
                    degree={MSc},
                % the dissertation    title (which cannot be blank)
                     title={Improving Text Classifier Performance through Human-in-the-Loop: Enhancing Learning from Explanations},
                % the dissertation subtitle (which can    be blank)
                  % subtitle={And those including an optional subtitle too, for good measure},
                % the dissertation     type
                      type={},
                % the year of submission
                      year={2022}]{dissertation}

\begin{document}

% =============================================================================

% This section simply introduces the structural guidelines.  It can clearly
% be deleted (or commented out) if you use the file as a template for your
% own dissertation: everything following it is in the correct order to use 
% as is.
% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).


\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures, tables and algorithms.  The former is a compulsory part of the
% dissertation, but if you do not require the latter they can be suppressed
% by simply commenting out the associated macro.

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Abstract}

\noindent
Text classification utilises Natural Language Processing (NLP) techniques to analyse pre-trained texts and assign them appropriate labels. Especially in crisis situations like floods, earthquakes, etc., text classifiers are crucial in identifying key information and effectively forwarding it to relevant agencies. However, the efficacy of text classification largely depends on abundant rich training data, which may be difficult to obtain in many scenarios \cite{mccreadie2019trec}. Relying on a vast amount of non-representative annotated data can lead to delays in project commencement and may impact the model's accuracy. Specifically, in emergencies, identifying action-related information (such as casualties or missing persons) becomes particularly challenging.\\

To address these issues, this paper introduces a Human-in-the-Loop (HITL) system integrated into the classifier's training process. Coupled with the representational engineering technique of natural language explanations—ExpBERT, a BERT fine-tuned on the MultiNLI natural language inference dataset is utilised to learn from explanations \cite{murty2020expbert}. This optimised embedding representation is used as input to the neural network classifier, further enhancing performance. The core task of the paper revolves around improving the performance of the text classifier based on ExpBERT by combining a limited but representative set of samples with labels and explanations provided by either humans or OpenAI \cite{baram2004online}.\\

This paper delves deeply into the application of active learning in text classification tasks based on ExpBERT. During the iterative process, acquisition functions based on uncertainty and diversity are used to select representative unlabeled instances for annotators to process. The original small sample and newly annotated data are used for model retraining. The results indicate that this method can achieve performance similar to models trained on vast datasets within a few iterations. To substantiate this, the model's performance using different active learning strategies is compared on the CrisisNLP dataset, and the effects of an active learning system combined with Monte Carlo Dropout (MCD) are also evaluated.\\

\noindent
The main conclusions of this paper are:
\begin{quote}
\noindent
\begin{itemize}
\item The Human-in-the-loop framework combined with a text classifier enables the model to achieve, or even surpass, the performance of models trained with the full dataset, using only a small amount of annotated data and undergoing a limited number of iterations.
\item Compared to merely annotating labels, low-quality explanations, or a limited number of explanations, annotating a certain amount of high-quality explanations can significantly boost the model's performance.
\item In terms of acquisition functions, semantic diversity-based sampling and Bayesian Active Learning by Disagreement (BALD) strategies combined with Monte Carlo Dropout (MCD) can allow the model to achieve higher average performance post-training.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------
\chapter*{Supporting Technologies}

\begin{quote}
\noindent
\begin{itemize}

\item I used Python as the development language.

\item I used the {\em Transformers} library to introduce pre-trained models and {\em Pytorch} for implementing neural networks.

\item I used OpenAI models as one of the annotation methods.

\item I used \LaTeX\ to format my thesis, via the online service {\em Overleaf}. 
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

\begin{quote}
\noindent
\begin{tabular}{lcl}
NLP                 &:     &  Natural Language Processing\\
BERT                 &:     & Bidirectional Encoder 
Representations from Transformers\\
ExpBERT              &:      &Representation Engineering with Natural Language Explanations\\
NLI                  &:      & Natural Language Inference\\
HITL                 &:      & Human-in-the-loop\\
AL                   &:      & Active Learning\\
BALD                 &:      &Bayesian Active Learning by Disagreement\\
MCD                  &:      &Monte Carlo Dropout\\
GPT                  &:      & Generative Pre-trained Transformer\\
NN                   &:      &Neural networks\\              
\end{tabular}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Acknowledgements}

\noindent
Xinyang Song would like to extend her profound gratitude to her supervisor, Dr. Edwin Simpson, for sharing literature and implementation examples related to ExpBERT and active learning, offering countless invaluable assistance. His professional guidance and enthusiastic support helped me overcome numerous challenges, complete my dissertation, and achieve meaningful insights.\\

Additionally, I'd like to thank the University of Bristol and my peers for their resources and emotional support during this time. My gratitude goes out to all of you.

% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter

% -----------------------------------------------------------------------------

\chapter{Introduction}
\label{chap:introduction}


% putting a \noindent before the first para in each chapter looks nicer.
\noindent
This chapter begins by introducing the research background of the text classifier based on ExpBERT integrated with Human-in-the-loop (Section \ref{section background}). Based on this background, the motivation for the experiment was formed in Section \ref{section motivation}, emphasising the significance of this research. Furthermore, a general overview of the work will be provided, contrasting with the limitations of traditional methods. Finally, the primary objectives and challenges are briefly summarised (Section \ref{section EM}).\\

\section{Background}
\label{section background}
\noindent
Emergency response systems for social platforms mainly focus on developing better text classification algorithms to learn from limited data. However, obtaining valuable annotated datasets can prove to be challenging \cite{prabhu2021multi}. The text classifier based on ExpBERT receives classification labels and corresponding explanations. These explanations detail which keywords led to a particular classification. ExpBERT incorporates this information into the model's training, allowing it to learn from deeper semantic insights to enhance its generalisation capabilities. Therefore, annotating with representative data and understanding the key phrases is vital for guiding the model towards accurate classification.\\

However, the data throughput of typical social platforms does not permit the annotation of tens of thousands of entries. Consequently, various forms of active learning that can sample high-information-value data at low costs have found extensive application in classification projects \cite{prest2011weakly, tuia2011survey}. Active learning emerges as an effective solution to these challenges, selecting a limited number of highly informative unannotated samples for human experts in the loop to annotate \cite{settles2009active, fu2013survey}. The newly data, once effectively annotated, dictates the performance of the model in the next iteration. Thus, employing different acquisition functions to query the most information-rich new instances is perhaps the most popular approach in active learning. Acquisition functions have naturally become the focal point of research in the domain of active learning.\\

Additionally, neural network-based text classifiers are often ill-suited for early uncertainty sampling \cite{schroder2020survey, ren2021survey}. This is because the weight parameters of the neural network are fixed values, causing the model to be overly confident in its predictions, both correct and incorrect. However, by employing dropout, it's possible to introduce a degree of uncertainty into the model. During training, the dropout layer randomly    ``turns off" a subset of input units. This randomness ensures that network weights are no longer fixed values, thus simulating weight uncertainty.\\

\section{Motivation}
\label{section motivation}
In light of the aforementioned background, this section emphasises the research motivation from three aspects: accuracy, representativeness, and robustness.\\

\subsection{Accuracy}
A unified feature of social networks, exemplified by Twitter, is the vast amount of data combined with internet-specific language syntax. The ultimate goal is to precisely classify urgent messages when capturing emergent needs and to dispatch relevant departments to address those needs. However, traditional text classification systems that achieve performance improvements are primarily done through extensive supervised learning, which is costly and slow to respond. Texts with varying amounts of information are randomly distributed, and the difference in information content can significantly impact model accuracy. Consequently, the results of capturing and identifying crucial emergency information are often subpar. Therefore, employing active learning models to utilise a small amount of data and enhance accuracy is urgently needed.\\

\subsection{Representativeness}
The ability to query rich feature representations from a large pool of unannotated data can mitigate much of the bias in multi-class active learning problems for emergency scenarios. By actively selecting samples for annotation, the model can choose the most valuable and representative samples in each iteration to enhance its performance. Therefore, selecting an effective acquisition function to seek out representative data has the most profound impact on performance and offers the most significant room for improvement.\\

\subsection{Robustness}
Original models often exhibit an overconfidence bias when handling text classification tasks. When encountering unfamiliar content, they make incorrect and unreliable predictions. In many practical applications, such as disaster response and medical diagnostics, erroneous predictions can have severe consequences. Thus, introducing a certain level of uncertainty to the model to curb its overconfidence has become an essential research topic. On the other hand, we can enhance the robustness of the model without introducing excessive computational overhead. This holds significant value for neural networks in scenarios that demand highly accurate and reliable predictions.\\

\section{Experimental Methodology}
\label{section EM}
Considering the background and motivations, this paper eventually designs a pool-based Human-in-the-loop active learning system for text classification. Unlike traditional text classification models, we train with a small amount of annotated data and then use the trained model in combination with various acquisition functions to select instances. Subsequently, preset annotations or human or OpenAI annotators observe and analyse the key features of the extracted instances in the loop, providing corresponding labels and adding a certain amount of explanations. These newly annotated instances are incorporated into the original data, and the above cycle is repeated until the performance reaches that of the full data (with the training set ratio close to 1, annotated with labels and default explanations).\\

The implementation will utilise uncertainty sampling based on Least Confidence, sentiment diversity sampling, and random sampling as a baseline to examine the performance under various active learning strategies. Detailed explanations of the acquisition functions will be provided in Chapter \ref{chap:background}. Furthermore, in every iteration, the concatenated explanatory texts will be processed using a pre-trained model based on ExpBERT to generate deep semantic representations. This embedded vector will then be input into the neural network text classifier.\\

Lastly, structural modifications will be made to traditional neural network(NN) models. Using the dropout mechanism, the model becomes more robust to minor variations in input, thereby enhancing its generalisation performance. Additionally, dropout can address the model's overconfidence issue by introducing noise and randomness, ensuring that model predictions aren't overly deterministic. While dropout doesn't directly quantify uncertainty, the introduced randomness and noise can bolster the model's resilience and, to some extent, alleviate its overconfidence. Given the high computational cost of entropy calculations in the original Bayesian Active Learning by Disagreement (BALD) algorithm, we've refined the BALD algorithm. Utilising Monte Carlo Dropout (MCD), we employ Least Confidence to select samples where the model's predictive probability is most uniform (i.e., without a particularly high predictive probability for any category).\\

\subsection{Objectives}
Based on the above experimental descriptions, the objectives of this paper can be summarised as follows:\\

\noindent
\begin{itemize}

\item Develop a pool-based Human-in-the-loop framework applied to the ExpBERT-based text classifier and investigate its effectiveness.

\item Construct an annotator simulation process and design stopping criteria (e.g., terminate when reaching the performance of the full data model).

\item Establish a baseline (performance of the full-data model without active learning or random sampling) and study whether active learning can enhance the performance of the text classification model. Compare the effects of different acquisition functions.

\item Explore the impact of the quality and quantity of explanations on this system. For instance, providing explanations with noise or increasing the number of explanations added in each iteration. 
\end{itemize}

\subsection{Challenges}
The main challenges of this project lie in the significant differences that may exist between various active learning strategies and how to improve the original initialisation framework of the multi-classifier based on ExpBERT and the annotation process to reduce computational costs.\\

\begin{itemize}
\item It's worth noting that there are multiple acquisition functions in active learning text classification systems. The application of different algorithms in active learning strategies significantly affects the performance of the active learning framework. The choice of strategy needs to consider both time complexity and representativeness.

\item Secondly, neural networks with dropout mechanisms might increase computational costs during training algorithm execution and when using MCD for active learning. Thus, how to reduce computational costs in each iteration to improve response efficiency poses a challenge.
\item Finally, regarding the framework design, determining the appropriate sampling ratio, the number of iterations, and the number of explanations provided in each iteration requires further study to achieve satisfactory performance.
\end{itemize}

% -----------------------------------------------------------------------------

\chapter{Background}
\label{chap:background}

\noindent
Chapter \ref{chap:background} introduces the relevant techniques centred around enhancing the performance of the text classifier based on ExpBERT through active learning. The ExpBERT mechanism used in the experiment is introduced (Section \ref{section 2.1}). ExpBERT will be applied during dataset pre-training. The active learning-based ``Human-in-the-loop" (Section \ref{section 2.2}) and its main acquisition functions (Sections \ref{section 2.3} and \ref{section 2.4}) are the focal implementation parts of this experiment, which can rapidly boost performance through this technique. Section \ref{section 2.5} describes the multi-model integration method, while Section \ref{section 2.6} discusses using OpenAI models to simulate the annotator role within the Human-in-the-loop. Lastly, the feasibility of applying Bayesian theory to active learning is concerned.\\

\section{ExpBERT}
\label{section 2.1}
\noindent
Representation Engineering with Natural Language Explanations (ExpBERT) proposes a method to enhance the control over the inductive biases of a model and maintains the expressive power of the representation \cite{murty2020expbert}. It combines fixed explanations provided by humans or, in this experiment, annotators with tweets to learn from these explanations and improve the model's performance. Figure \ref{fig1} intuitively shows how to combine samples with explanations with BERT fine-tuned on MultiNLI \cite{williams2017broad}. Explanations play a key role here. The quality of the explanations has a significant impact on the performance of ExpBERT. By using high-quality explanations, the model's learning can be guided.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Figure/BERT.png}
    \caption{ExpBERT to produce representations that form the input to the
classifier}
    \label{fig1}
\end{figure}

First, each tweet is fully connected with a set of pre-written explanations to generate a 3$\times$3 set. These explanations are unrelated to specific tweets; each tweet will be connected with the same number of explanations. The preprocessed text and explanations are input into BERT to generate features that ``interpret" each explanation \cite{devlin2018bert}. The classifier can be trained to classify the representations of the tweets and explanations.\\

After being fine-tuned on the MultiNLI dataset, the BERT model will generate a feature vector for each input sample, representing the entire input of a length of 768. The feature vectors of tweets and explanations are then connected to form an embedding with a size of 768$\ast$E, where E is the number of explanations. This will be used as the input data for training and predicting in the classifier model. This paper will use this model as the basic pre-trained model. At the same time, combined with Natural Language Inference (NLI) technique can be used to reduce the number of embeddings (the size will be 768 + (3$\ast$E)) and integrate and initialise them into the HITL system \cite{maccartney2009natural}. Because this model can handle instance vectors with explanations, the optimised embedding representation as input to the classification model can improve the performance of the model.\\

\section{HITL}
\label{section 2.2}
Traditional natural language processing workflows are not designed to fully leverage human feedback. In contrast, Human-in-the-Loop (HITL) serves as an essential component of interactive systems, revealing potential model flaws through simulating human roles within the loop. These flaws might not be readily apparent before real-world testing \cite{tomaszewski2021overview}. Godbole et al. (2004) \cite{godbole2004document} expanded a text classifier employing Support Vector Machine (SVM) active learning. Ingeniously, they integrated human input on aspects such as feature engineering, terminology selection, and document labelling into the model, allowing statistically sound judgments to be made. This innovative mode of interaction between humans and machine learning algorithms is termed ``Human-in-the-Loop (HITL) machine learning" \cite{monarch2021human}. In this approach, various HITL machine-learning schemes can be determined based on different needs and contexts, driven by different types of collaborations between humans and machines \cite{mosqueira2023human}.

\paragraph {Active Learning (AL)} \cite{settles2009active}: The essence of active learning lies in the system's control over the model's learning process. Although humans serve as intermediaries to annotate unlabeled data, they cannot choose unlabeled data based on preference. AL will be applied as an optimisation framework in this paper, with a detailed exploration of its application in Section \ref{section 2.3}.

\paragraph{Interactive Machine Learning (IML)} \cite{amershi2014power} is a method where humans maintain close interaction with the system. Within this context, there are notable distinctions between Active Learning (AL) and Interactive Machine Learning (IML). Dudley and Kristensson (2018) \cite{dudley2018review} highlighted that although both AL and IML prioritise the user's selection of new data points for labelling, they are driven by different motivations. Specifically, in AL, the selection is propelled by the model itself, while in IML, the choice is driven by the user. Given that IML has evolved from the foundations of AL, they share several common drawbacks. However, IML also faces unique challenges, especially those related to Human-Computer Interaction (HCI) complexities. These distinct challenges necessitate more specialised research to address, as indicated by Michael et al. (2020) \cite{michael2020interactive}.

\paragraph{Machine Teaching (MT)} \cite{ramos2020interactive} is an approach where machine learning models are trained under the guidance of human educators. More precisely, human teachers define the knowledge they aim to convey to the model, underscoring their active involvement and direction throughout the learning process. Devidze et al. (2020) \cite{devidze2020understanding} pointed out that, compared to active learning, MT heavily leans on the teacher's expertise and offers less flexibility in sample selection and handling intricate tasks. Hence, selecting the right learning method becomes paramount based on the specific requirements.\\

\section{Active Learning}
\label{section 2.3}
Within the context of HITL, active learning systems, as one of the most popular learning approaches, aim to overcome the labelling bottleneck by posing questions to unlabeled instances and having them labelled by experts, such as human annotators \cite{aggarwal2014active, settles2009active}. In essence, active learning seeks to identify the most informative unlabeled raw data and present it to annotators for labelling. This process closely mirrors real-world scenarios of extracting information from source data. Annotators label this source data, and the labelled instances are incorporated into the model's training process. In this way, active learning manages to achieve performance comparable to using the full dataset but with fewer labelled data, thereby addressing the labelling bottleneck. The active learning approach proves especially pertinent given this project's vast number of instances, coupled with significant noise, cumbersome full-scale training tasks, and suboptimal accuracy. Thus, active learning is apt for this project, focusing on instances that might enhance accuracy, minimise irrelevant instances, and boost the learning efficiency and precision of the emergency response system.\\

\subsection{The AL Process and Scenarios}
\label{section AL Process}
\noindent
The active learning process is illustrated in Figure \ref{fig2}, which depicts the process of pool-based active learning. The machine learning model is initially trained using the labelled instance set $L$. Afterwards, the model gives the score to the unlabeled dataset and selects representative unlabeled samples based on the acquisition function, presenting them to human annotators. These annotators label the chosen samples, which are then removed from the unlabeled sample set $U$ and added to the labelled set $L$. Through iterative execution of the steps above, the quantity of labelled samples grows incrementally, enhancing the model's performance. The active learning process will conclude once the stopping criterion is met, such as achieving performance metrics on the entirety of the data set.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Figure/AL.png}
    \caption{Pool-based active learning process}
    \label{fig2}
\end{figure}
\\

Based on the classification by Settles (2009) \cite{settles2009active}, active learning primarily encompasses three scenario settings: Membership Query Synthesis \cite{angluin1988queries, king2004functional}, Stream-based Selective Sampling \cite{dagan1995committee}, and Pool-based active learning \cite{lewis1995sequential}.

\paragraph{Membership Query Synthesis:} In this active learning scenario, the strategy is generated by the model itself. The machine learning model can request labels for any unlabeled instance that hasn't been sampled from the underlying natural distribution \cite{settles2009active}. This query approach is typically effective for more deterministic problem domains, such as tasks involving regression predictions on absolute coordinates. It can interpret straightforward data distributions and construct viable data for human annotation. However, as evidenced in the study by Baum and Lang (1992) \cite{lang1992query}, in complex tasks like natural language processing, the model might produce text strings that are hard to comprehend, rendering human judgment on these jumbled texts challenging. Within the context of deep active learning, one can address the scenarios of Membership Query Synthesis through Generative Adversarial Networks (GANs) for data augmentation, as GANs are capable of generating instances with a high degree of plausibility \cite{goodfellow2020generative}.

\paragraph{Stream-based Selective Sampling:} The distinction of this setup in comparison to Membership Query Synthesis lies in its ability to operate irrespective of the input distribution. Stream-based Selective Sampling entails drawing a single unlabeled instance from the actual distribution at a time. The model then determines whether to request the label of the instance through ``information measures" or ``query strategies", essentially acting as a kind of biased random sampling \cite{dagan1995committee}.

\paragraph{Pool-based active learning:}
As a commonly employed setting in active learning, the difference between pool-based active learning and stream-based Selective Sampling is that the former employs a greedy mechanism before selecting the best query, comparing the entire dataset. In contrast, stream-based sampling assesses data as it comes in on an individual basis. However, dealing with vast datasets could result in lengthy computational times due to the need to evaluate all the data. Nonetheless, this method is particularly suitable for scenarios similar to this project where manual annotation costs are significant. By choosing the most valuable data for annotation, it aims to maximise the return on the annotation investment.\\

\subsection{Acquisition Functions}
\label{section 2.3.2}
This section considers various acquisition functions in active learning. In the design of the paper (Chapter \ref{chap:design}), we will utilise a text multi-classification model based on ExpBERT combined with different acquisition functions to enhance performance.

\subsubsection{Random Sampling}

Random sampling involves selecting instances independent of predictions, data, or model considerations. It's commonly used as a task benchmark since it might overlook potential informative instances, reducing learning efficiency. In this context, random sampling will serve as a baseline to contrast with the more sophisticated strategies discussed below, especially when the annotation pool becomes overly extensive \cite{sener2017active}.

\subsubsection{Prediction Uncertainty Sampling}

Uncertainty sampling is a specific active learning strategy that queries instances most difficult for the model to classify. By annotating these challenging instances, the model's accuracy is enhanced. In probabilistic models for binary classification, such instances have posterior positive probabilities close to 0.5 \cite{lewis1995sequential}. However, for more complex multi-label classification tasks, entropy-based methods are used. The more uniform the probability distribution, the greater the entropy. As entropy increases, the uncertainty or informational content of the random variable becomes higher. Conversely, when probabilities are concentrated on a few data points, the uncertainty is lower and the informational content is smaller.\\
\begin{equation}
    x_{\mathrm{ENT}}^*={\arg \max _x} - \sum_i P\left(y_i \mid x ; \theta\right) \log P\left(y_i \mid x ; \theta\right) 
    \label{equation 2.1}
\end{equation}
$P\left(y_i\right)$ denotes the probability distribution at classification $i$.\\

In the domain of text classification, an alternative method often used to measure uncertainty is the ``Least Confidence(LC)" \cite{hu2016active}:\\
\begin{equation}
    x_{L C}^*={\arg \min _x} P\left(y^* \mid x ; \theta\right), y^*=\arg \max _y P(y \mid x ; \theta) 
    \label{equation 2.2}
\end{equation}
$y^*$ represents the label of the class with the highest likelihood. This method is equivalent to the effectiveness of entropy-based algorithms for binary text classification. It focuses on samples for which the model predicts the highest probability but with low confidence. Specifically, the method involves annotating the samples with the smallest maximum probability. This more comprehensive consideration will be adopted in this paper.\\

In addition to the two commonly used measurement methods mentioned above, Munro \cite{monarch2021human} introduced the margin of confidence and ratio of confidence. The former refers to the difference between the two most confident predictions, while the latter is the ratio between these two predictions.\\
\begin{equation}
    \mathrm{margin\_conf} = 1-\left(P\left(y_1^* \mid x\right)-P\left(y_2^* \mid x\right)\right), \quad \mathrm{ration\_conf} = \frac{P\left(y_2^* \mid x\right)}{P\left(y_1^* \mid x\right)} 
    \label{3}
\end{equation}
$y_1^*$ is the most confident, $y_2^*$ is the second most confident.

\subsubsection{Query-By-Committee}

Compared to uncertainty sampling, which evaluates the judgement of a single model, the Query-By-Committee (QBC) approach employs multiple models to act as ``members" of a committee \cite{seung1992query}. These members are trained on the available labelled data using the Gibbs algorithm. The committee then randomly selects a hypothesis consistent with the current labelled data. The subsequent query is picked based on the instance where the committee members exhibit the most significant disagreement \cite{burbidge2007active}. Hence, while uncertainty sampling assesses the discernment of an individual member (model), QBC often incorporates strategies such as using vote entropy to select instances that are challenging for models to agree upon \cite{dagan1995committee}. And choosing samples with a high average Kullback-Leibler (KL) divergence \cite{mccallum1998employing}.\\

\textbf{Vote entropy:} For text classification tasks, each committee member (text classifiers) can vote on unlabeled samples. 
\begin{equation}
    H(V) = -\sum_{i=1}^{n}\frac{Vote(y_{i})}{C}\text{log}\frac{Vote(y_{i})}{C}
\end{equation}\\
$Vote(y_{i})$ represents the votes received for class $y_{i}$, and the total number of votes is $C$, which is also the total number of classifiers. The larger the vote entropy, the more controversial the sample is.\\

\textbf{Average KL Divergence:} The KL divergence increases as the difference between two distributions grows. The formula is expressed as:
\begin{equation}
    x_{KL}^{*} = argmax_{x}\frac{1}{C}\sum_{c=1}^{C}D_{KL}(P_{c}||\overline{P})
\end{equation}
Where,\\
\begin{equation}
    D_{KL}(P_{c}||\overline{P})=\sum_{}^{}iP_{c}(i)\text{log}\frac{P_{c}(i)}{\overline{P}(i)}
\end{equation}
$P_{c}$ is the prediction probability distribution of the $ith$ committee member, and $\overline{P}$ is the average prediction probability distribution of all members.\\

It is noteworthy that Query-By-Committee (QBC) requires simultaneous operation of multiple models. Hence, one drawback of methods based on QBC is their slower speed, which is a factor of the committee's size \cite{bloodgood2018support}. Speed is paramount for active learning applications, but the trade-off with QBC is that annotators experience prolonged waiting times while the system identifies instances using QBC.

\subsubsection{Semantic-based Diversity Sampling}

Peng, Hao, et al. \cite{peng2023query} introduced a semantic-based diversity sampling method that can be applied in text classification combined with active learning in this experiment. Distinct from the uncertainty sampling approach, the semantic-based diversity sampling method eliminates text sample redundancy semantically using the \emph{Euclidean distance}. This distance measure is frequently used across various domains, including data clustering and nearest neighbour search in embedding spaces. In machine learning, calculating a Euclidean distance from an embedding to a cluster centre can be achieved by computing a distance matrix for every embedding to every cluster centre, where ``$distances[i, j]$" represents the Euclidean distance from the $i$th embedding to the $j$th cluster centre \cite{bishop2006pattern}.\\

To ensure that richer and less redundant samples are provided to the model (learner) in subsequent processes, this abstraction method employs the greedy k-centre clustering algorithm by Sener and Saveravarese (2017) \cite{sener2017active}. The dataset $D$ contains $n\times m$ unlabelled texts and divides $D$ into $n$ batches, with each sample set containing $m$ instances. It is the result of encoding the dataset. First, select $\alpha$ vectors from $Y_{i}^{*}$ to initialise the clusters $c_{i}^{0}$. Here examples will be considered as cluster centres. Then the k-centred algorithm searches $u_{i}^n$ from $\widetilde{c_{i}^0}$, which is a set that includes members that are not in $Y_{i}^{*}$. It is the furthest from the centre of all clusters. The algorithm chosen is formulated as follows:
\begin{equation}
\begin{aligned}
u_i^0={\arg \max _{y_i^k \in \widetilde{c}_i^0}} {\min _{y_i^j \in c_i^0}} \left\|y_i^k-y_i^j\right\|_2^2 
\\
u_i^1=\arg \max _{y_i^k \in \widetilde{c}_i^1} \min _{y_i^j \in c_i^1}\left\|y_i^k-y_i^j\right\|_2^2
\end{aligned}
\end{equation}
Where:
\begin{equation}
\begin{aligned}
& \widetilde{c}_i^0=Y_i^* \backslash c_i^0 \\
& c_i^1=c_i^0 \bigcup u_0
\end{aligned}
\end{equation}
This is followed by updating the existing clusters to $c_{i}^{1}$ after a loop execution and merging the output into $P$. All text instances in $P$ converge into a core set that best represents and generalises the dataset $D$ in the semantic space. 

\subsubsection{Bayesian Active Learning by Disagreement}

Bayesian Active Learning by Disagreement (BALD) is an active learning strategy suitable for text classification problems, combining Bayesian inference with uncertainty measures to select information-rich samples. In BALD, uncertainty is measured by calculating the inconsistency in predictions for each sample's class probability distribution, as proposed by Houlsby, N. (2011) \cite{houlsby2011bayesian}. The learner (model) aims to maximise the uncertainty of the model parameters through the input $x$. $\mathrm{H}(y \mid x, D)$ represents the uncertainty of the target variable. The second term of the equation represents the expected uncertainty (entropy) of $\mathrm{H}[y \mid x, \theta]$, given that the parameter $\theta$ follows the posterior probability distribution $p(\theta \mid D)$ based on the training dataset $D$, measuring the average uncertainty.\\
\begin{equation}
x^*=\underset{x}{\arg \max } \mathrm{H}(y \mid x, D)-\mathrm{E}_{\theta \sim p(\theta \mid D)}[\mathrm{H}[y \mid x, \theta]] \mid
\end{equation}\\

To better calculate the inconsistency between predictions, Gal et al. proposed a specific sampling function in 2017 that utilises the Monte Carlo Dropout (MCD) method \cite{gal2017deep}. MCD is a regularisation technique during training, which, by randomly deactivating certain neurons, simulates the posterior distribution of Bayesian networks \cite{myojin2020detecting}. This allows for selecting samples with the highest uncertainty for labelling by making multiple predictions for each sample and calculating the inconsistency between the model's predictions.\\

At its core, MCD is a regularisation method designed to prevent overfitting by probabilistically deactivating a subset of neurons. During the inference process, different results are produced during the prediction phase by inputting into the neural network a finite number of times (T times) and using dropout \cite{tsymbalov2018dropout}. Consequently, after quantifying the uncertainty from these varying prediction results, they can be used to estimate the model's uncertainty about unlabeled samples. This is then applied in the Bayesian Active Learning by Disagreement (BALD) sampling strategy to select the most uncertain unlabeled data. The inference results of MCD are derived in the following manner:
\begin{equation}
    \overline{y}(x)\approx \frac{1}{N_{mc}}\sum_{}^{N_{mc}}y_{drop}(x)
\end{equation}
Wherein, $y_{drop}$ represents the varying outputs from the dropout-enabled network, $x$ is the input to the network, and $N_{mc}$ is the number of samples required to obtain the distribution. Ultimately, BALD selects the samples with the highest informational gain by comparing each sample's BALD value. These samples are then labelled and added to the annotated training set. Moreover, BALD can be paired with the previously mentioned \emph{Least Confidence}, replacing the original entropy computation, to select samples for which the model prediction probabilities are most uniform (i.e., samples without a particularly high prediction probability for any category).\\

\section{Small-Text Library}
\label{section 2.4}
The text classifier in this project tends to focus on a single model, potentially overlooking the applicability of other feasible models. However, the time-consuming of switching models and active learning strategies, coupled with code redundancy, would significantly impact the progress of the experiment. The Small-Text Library integrates commonly used libraries like scikit-learn, transformers, and PyTorch that can be applied within the Python environment \cite{schroder2021small}.\\

The pool-based active learning framework for text classification bridges the interfaces of active learning strategies, classifiers, and stopping criteria. It provides an advanced active learning framework for text classification tasks and offers a range of classifier and acquisition function components. These can be combined in various permutations for experimental and applied active learning tasks, facilitating the implementation of active learning in the Python ecosystem. Compared to the popular ModAL library \cite{danka2018modal}, Small-Text offers more flexible customisation services, with the former focusing on model ensembles and strategy choices. Nonetheless, this all-encompassing approach might not necessarily enhance the performance of the ExpBERT-based project, as performance discrepancies can vary based on circumstances.\\

\section{GPT Annotator}
\label{section 2.5}
GPT (Generative Pre-trained Transformer) is a pre-trained language model developed by the OpenAI team in 2018 \cite{azaria2022chatgpt}. Its key algorithm is the Transformer, a deep neural network structure based on the self-attention mechanism, boasting potent sequence modelling and representation learning capabilities \cite{lin2022survey}. This model can analyse and generate natural language text through pre-training and fine-tuning, proving beneficial in multiple scenarios such as automatic answering, intelligent customer support, and language translation. \\

As a result, for a time-saving and efficient text classification system, this project will employ this model as an active learning annotator based on ExpBERT. The primary task is to generate appropriate explanations for ExpBERT and provide them to the model to enhance classification performance. In this configuration, active learning generates human-readable explanations for input text by leveraging GPT's robust generative model \cite{shi2023chatgraph}. \\

A common approach involves using GPT to find keywords from the sampled text of a specific category, combine the keywords to form explanations, and then choose one or more to integrate into the original explanations. The combined text is then passed to the ExpBERT model. By analysing the sampled text in this manner, one can overcome deficiencies in human memory and the inadequacy of semantic similarity computation, ultimately identifying crucial feature words and phrases that best assist the model in understanding the intrinsic structure of the data.\\

\section{Bayesian Neural Networks in Active Learning}
\label{section 2.6}
In modern machine learning and deep learning applications, Bayesian Neural Networks (BNNs) have emerged as critical components due to their flexible inference mechanisms. BNNs offer a rich and adaptable modelling framework by introducing uncertainties in weights and predictions \cite{mackay1992practical}. This not only bolsters the model's reliability but also enhances its ability to generalise, adapt to noise, and handle outliers. Unlike traditional neural networks where weights are fixed, for BNNs, they are variable, as shown in Figure \ref{fig3}. The idea behind the model is to combine the strengths of neural networks with probabilistic modelling, providing probabilistic assurance for predictions \cite{mullachery2018bayesian}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Figure/BNN.png}
    \caption{Traditional Neural Networks and Bayesian Neural Networks}
    \label{fig3}
\end{figure}
\\

The weights in this context are inferred based on the observations we have, presenting an inverse probability problem solved through Bayes' theorem \cite{goan2020bayesian}. As shown in the equation, the weight $\omega$ represents latent variables for which the true distribution is unseen. Bayesian inference allows us to obtain the distribution of model parameters
conditioned on the observed data, $p(\omega \mid D)$, known as the posterior distribution. 
\begin{equation}
    \pi(\omega \mid D) = \frac{p(\omega)p(D \mid \omega)}{\int_{}^{}p(\omega)p(D \mid \omega)d \omega}=\frac{p(\omega)p(D \mid \omega)}{p(D)}
\end{equation}\\
Additionally, the likelihood function $p(D \mid \omega)$ in multi-class problems combines the neural network's predictions (after a Log Softmax transformation) with the actual observed class labels to quantify their consistency. The posterior distribution can be computed using Bayesian theory after determining the likelihood and the prior distribution $p(\omega)$.\\

After obtaining the posterior distribution, marginalisation methods will be used for prediction:
\begin{equation}
    \mathbb{E}[f]=\int_{}^{}f(\omega)\pi(\omega \mid D)d \text{ }\omega
\end{equation}
\\

Bayesian Neural Networks provide a way to quantify uncertainty, enabling active learning systems to identify instances on which the model is least confident, subsequently selecting these instances for annotation. However, integrating the model parameters often requires computationally expensive operations, such as Markov Chain Monte Carlo (MCMC) sampling \cite{hastings1970monte}. This can become problematic when dealing with large-scale datasets, especially in the context of active learning, which typically requires multiple iterations and model updates. Therefore, the actual implementation of BNNs may prolong the initialisation time of emergency systems \cite{neal2012bayesian}.\\

% -----------------------------------------------------------------------------

\chapter{Design}
\label{chap:design}
\noindent
This chapter will provide an overview of the active learning framework in Section \ref{section 3.1}, followed by a detailed breakdown and design according to the sequence of tasks in active learning. Starting with pre-training the data in Section \ref{section 3.2}, we then discuss the design of classifier models that acquire embedded representations in Section \ref{section 3.3}. Section \ref{section 3.4} presents the design of active learning strategies from multiple perspectives using the trained classifier model and analyses the implementation and effects of different acquisition functions. Finally, the design of the annotator's process functionality is covered in Section \ref{section 3.5}.\\

\section{Structure Overview}
\label{section 3.1}
The overall experimental framework is built upon the pool-based active learning technique discussed in Section \ref{section AL Process}. The system samples from a fixed pool of unannotated data using a trained model for sample assessment. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Figure/structure.png}
    \caption{The overall experimental framework}
    \label{fig4}
\end{figure}
\\

As depicted in Figure \ref{fig4}, in contrast to traditional active learning methods, this study revises the dataset requirements, introducing two distinct datasets: \textbf{one annotated with labels and explanations} (exp\_label\_dataset) and \textbf{another neither labelled nor explained} (no\_exp\_label\_dataset). The `exp\_label\_dataset' encompasses data for training, validation, and testing. However, during actual testing and validation, the model only accesses explanations and text embeddings without the corresponding labels. Both explanations and labels are essential during training. To harness the full capabilities of the pre-trained ExpBERT model, each annotation cycle not only provides labels but also appends new explanations to the default explanation set (Explanation.txt). \\

Throughout the process, three main sets continuously engage in iterations within three primary modules until the evaluation score based on the \textbf{validation set} approaches or exceed the evaluation score for the full data volume.These modules are training, annotation, and updating:

\paragraph{Training:} During the initial training phase, \textbf{18\%} of the entire data volume, which comes with labels and explanations, is used. Each sample in the `exp\_label\_dataset' is linked to 9 default explanations (label descriptions) for the first round of training. After the first round of training, the \textbf{trained model} and the `no\_exp\_label\_dataset' are used as input parameters for the active learning strategy. The ``information content" of each sample is then calculated from perspectives such as uncertainty or diversity. Samples with the highest ``information content" are subsequently selected for annotation.

\paragraph{Annotation:} In each iteration, the annotator receives \textbf{4\%} of the samples. During the annotation process depicted in the figure, the annotator identifies key phrases in the text that \textbf{semantically resemble} the category description as explanations, and the system automatically provides the classification label for each text. Once samples are fully annotated, they are added to the `exp\_label\_dataset', forming the dataset required for the next iteration.

\paragraph{Dataset Update:} Subsequently, the annotated data, which constitutes \textbf{4\%} of the total data volume, is removed from the `no\_exp\_label\_dataset'. With each iteration, the size of the `exp\_label\_dataset' increases by \textbf{4\%}, and one or more explanations are added. After updating the `Explanation.txt', the total number of explanations will exceed the initial default set. All these datasets are pre-trained by the ExpBERT-based model before being fed into the neural network text classifier. \\

\section{Pre-training}
\label{section 3.2}
According to the framework design mentioned above, all datasets require a pre-training process before training the text classifier. As outlined in Section \ref{section 2.1} about ExpBERT, pre-trained models are typically trained on a large volume of text data, capturing deep bidirectional language representations. As a result, they often achieve higher performance on specific tasks. In many text classification scenarios, using a pre-trained model as a foundation and fine-tuning it can significantly reduce the training time of the text classification model. \\
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{Figure/BERT2.png}
    \caption{Overview of pre-training process}
    \label{fig5}
\end{figure}
\\
Text is typically input into the model to acquire its embedding representation. This embedding representation can then serve as the input for downstream text classifiers. However, the embedding size obtained from the original model is often too large, leading to extended computation times. Therefore, as illustrated in Figure \ref{fig5}, Natural Language Inference (NLI) is employed to fine-tune BERT, resulting in an embedding size of 3$*$E for each sample. To retain tweet information, after concatenating the text embedding representations, the size of a sample's embedding is only \textbf{768 plus 3$*$E}.\\

In addition, it is assumed that the number of explanations corresponding to the annotated dataset will increase by `new\_exp' during the active learning iterations. Therefore, every time the iteration reaches the pre-training step, it is necessary to re-connect the updated training set, test set, and validation set texts with all the updated explanations as input to the pre-training model. As a result, each pre-training session will handle an additional \textbf{num\_tweets$*$new\_exp} amount of data. This aspect also contributes to the extended duration of the experiment implementation.\\

For the unannotated dataset, to simulate real-world conditions, only the text data needs to be processed during pre-training. However, the trained neural network expects to receive input with the same dimensions as during training. Thus, for the embedding representation of unannotated data, the `F.pad(embedding, (0, pad\_amount))' method is used to ensure the tensor dimensions of the embedding match. Lastly, pre-training employs a pool-based approach to speed up the computation to enhance the computational efficiency of each iteration. The `Pool' class from Python's `multiprocessing' library is utilised to process data batches in parallel. Multiple batches can be processed simultaneously on multi-core machines, allowing for flexible process control and increased processing efficiency.\\

\section{Classifier Model}
\label{section 3.3}
This paper employs neural networks as the classifier model, taking the post-pre-training embeddings as input. The evaluation phase (Chapter \ref{chap:evaluation}) will compare the performance of conventional neural network classifiers and neural network classifiers with `dropout' in conjunction with active learning techniques. Structural modifications will be made to the traditional neural networks in this experiment. The final model structure will be built using PyTorch \cite{imambi2021pytorch}.\\

\subsection{Before Dropout}
Regarding model architecture, the neural network in this study employs a single hidden layer and 100 neurons. The output feature count, or `output size', is 9 (number of labels). The forward propagation design can be represented by the following formula, where $W_{i}$ and $b_{i}$ are the weights and biases of the linear layer, respectively. After passing through an activation function, the model output $y$ is obtained. This study will use the $ReLU$ non-linear activation function, which induces network sparsity, alleviating overfitting issues \cite{sharma2017activation}.
\begin{equation}
    h = W_{i}x+b_{i},\quad a = ReLU(h),\quad y = W_{i+1}a+b_{i+1}
\end{equation}
Regarding training methods, the conventional neural network uses the standard loss computation method (cross-entropy \cite{de2005tutorial}). Additionally, the steps of forward propagation, loss computation, backward propagation, and optimisation are executed separately using PyTorch's standard methods.\\

\subsection{After Dropout}
This paper has improved the model structure by employing a dropout neural network model as a text classifier when applying the BALD method. Hinton mentioned that Dropout can prevent neurons from overly relying on other specific neurons in the network, thereby enhancing their independence and strengthening the model's generalisation capability \cite{hinton2012improving}. Applying dropout to neural networks is equivalent to having a ``sparse" network made up of units that have survived the dropout. A neural network with n units can have $2^n$ possible sub-network configurations. For each presented training instance, a new sparse network is sampled and trained \cite{srivastava2014dropout}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{Figure/dropout.png}
    \caption{Standard Neural Network and Dropout Neural Network.}
    \label{fig6}
\end{figure}\\

Dropout effectively approximates different neural network structures by randomly discarding units. The dropout process is illustrated in Figure \ref{fig6}, achieved by removing some neurons from the network and breaking their connections. In this paper, the `dropout\_prob' parameter is set to \textbf{0.2} during the initialisation of the neural network, meaning that the probability of a neuron being temporarily removed from the network during each forward propagation is 20\%. At this point, the forward propagation formula for the original neural network changes:
\begin{equation}
a = ReLU(h),\quad r = Bernoulli(p), \quad aDropout = r*a
, \quad y = W_{i+1}*aDropout+b_{i+1}
\end{equation}
$Bernoulli(p)$ represents a random variable following the \textbf{Bernoulli distribution}, used to generate a probability vector $r$, which randomly yields a vector of 0s and 1s. Consequently, some elements in vector $a$ are set to 0 based on this distribution.\\

\section{Active Learning Strategy}
\label{section 3.4}
In Section \ref{section 3.1}, the acquisition function process was mentioned. In the design, \textbf{four different active learning strategies} were adopted to try to improve model performance: random sampling as a baseline; uncertainty sampling based on Least Confidence; diversity sampling based on sentiment; and Bayesian Active Learning by Disagreement (BALD) based on MCD and Least Confidence. Each of these strategies explores and analyses the characteristics of the data in different ways, serving as the core of the active learning framework to enhance the efficiency and accuracy of model learning. The best ways to improve model performance are explored by sampling from different perspectives.

\subsubsection{Random Sampling}

During those sampling processes, the acquisition function will evaluate nearly 70\% of the unannotated dataset, and each evaluation provides 4\% of the unannotated dataset to annotators for annotation. Random sampling ensures \textbf{unbiased} sample selection. The sampling is done directly on the unannotated dataset using Python's built-in random module without taking the trained model into account. The model performance under its influence will serve as a \textbf{baseline}.

\subsubsection{Uncertainty Sampling Based on Least Confidence}

This paper employs the Least Confidence approach for uncertainty estimation. As discussed in Section \ref{section 2.3.2}, the Least Confidence method is often used as an alternative to entropy computation for multi-label text classification problems. In this experiment, the model is set to evaluation mode. The \textbf{highest probability} value is sought within the probability distribution 'prob\_dist' given by the model. The number of labels 'num\_labels' is calculated, and the most confident prediction is normalised using the formula below. The resulting 'normalized\_lc' provides a standardised score where a \textbf{higher value indicates less confidence} in the sample.
\begin{equation}
\text{normalized\_lc} = (1 - \max(\text{prob\_dist})) \times \left( \frac{\text{num\_labels}}{\text{num\_labels} - 1}\right)
\end{equation}
Samples are ranked in ascending order based on this score, and the top 4\% of samples with the \textbf{highest scores} are selected as the output for this uncertainty method.

\subsubsection{Semantic-based Diversity Sampling}

In this study, a semantic diversity-based sampling method is employed. This approach leverages a perspective distinct from uncertainty sampling in the active learning process to identify enriching and minimally redundant samples. For the dataset under consideration, there is a significant imbalance in the number of samples across categories. Some categories account for only 2\% of the entire dataset. Such imbalances might lead to certain categories being under-trained, resulting in considerable uncertainty during evaluation. Relying solely on uncertainty for sampling may lead to the over-sampling of a specific category, thereby skewing the distribution of the sampled results, consequently affecting the learning outcome of the model. Hence, a semantic diversity-based sampling strategy is adopted to utilise samples from all categories during active learning effectively.\\

This method requires loading an unannotated dataset and using the embeddings from a pre-trained model. By setting the trained neural network model to evaluation mode, probabilities corresponding to the input embeddings are computed. As introduced in Section \ref{section 2.3.2}, each text's embedding vector is denoted as $Y_{i*}$, where $i$ represents the index of the text. Suppose the dataset is divided into $n$ batches, with $m$ instances each. Initially, a random subset of embedding vectors is chosen as the clustering centres, denoted as $C_{i*}$. In this study, $C_{i*}$ is represented as follows, where the proportion of the initial clustering centres, or $\alpha$, is set to 0.2:
\begin{equation}
    C_{i*}=\text{random\_choice}(Y_{i*},\alpha*n*m)
\end{equation}
Subsequently, the greedy k-centre algorithm is utilised to search for embedding vectors in Yi that aren't in Ci*. The algorithm identifies embedding vectors that are the \textbf{furthest from the existing clustering centres} and updates the set of clustering centres. This process is illustrated as:
\begin{equation}
    C_{i*} = \text{argmax}_{y\in Y_{i*}\backslash C} \text{min}_{c\in C_{i*}}\text{dist}(y,c)
\end{equation}
The `dist' function computes the \textbf{semantic distance} between embedding vectors and clustering centres. Finally, we return the indices in $Y_{i*}$ for each cluster centre in $C_{i*}$ denoted as $O_i$. 

\subsubsection{MCD Least Confidence BALD}

This study adopts a sampling strategy based on Monte Carlo Dropout(MCD) to find samples with higher model uncertainty for more effective model training. The foundation of this strategy is a study by Gal and Ghahramani in 2016 \cite{gal2016dropout}, where they proposed using dropout during neural network training as an approximation to Bayesian inference. Similar to other active learning, the embedding representation of the unlabeled dataset is loaded. \\

\begin{algorithm}[H]
\renewcommand{\thealgocf}{3.1}
\caption{BALD using MCD}\label{algorithm}
Set model to TRAINING mode\;
\For{$input\_data$ {\bf in} Embeddings}{
    Initialise predictions as an empty list\;
    \For{$j = 1$ {\bf upto} T\_samples}{
        $ \text{logits} \leftarrow \text{model.forward(input\_data)}$\;
        probabilities $\leftarrow$ SOFTMAX(logits)\;
        APPEND probabilities to predictions\;
    }
    avg\_probabilities $\leftarrow$ AVERAGE(predictions)\;
    bald\_score $\leftarrow$ NEGATIVE MAXIMUM of avg\_probabilities\;
    APPEND (input\_data's index, bald\_score) to selected\_indices\;
}
SORT selected\_indices by bald\_score in DESCENDING order\;
\Return top k indices from selected\_indices\;
\label{ok}
\end{algorithm}
\hspace*{\fill}\\

However, the difference is that the text classification model is a neural network with dropout. Also, as shown in Algorithm 3.1, it needs to be \textbf{set in training mode}, not evaluation mode. This is because MCD requires the dropout to be active during training. Unlike the traditional BALD strategy, the experiment measures uncertainty by \textbf{maximising the negative maximum of the prediction probability} rather than the conventional BALD calculation method. Next, the indices of all samples and their corresponding BALD scores are stored in a list and sorted in descending order based on the BALD scores. In this way, we can identify the samples for which the model is most uncertain about their classification, i.e., samples with the \textbf{highest} BALD scores. This method can be seen as a variant of the BALD algorithm that integrates MCD and the Least Confidence strategy. Compared to uncertainty sampling that only uses the Least Confidence, this method can capture \textbf{more stable uncertainty information} from the model.\\

\section{Annotation Process}
\label{section 3.5}
After the sampling guided by the active learning strategy, a set of unannotated texts with high information content is obtained. For these texts, annotators need to provide the corresponding classification based on their content and decide which explanatory notes to add in the current iteration.\\

\subsection{Label Annotation}
The focus of this experiment is to explore whether the addition of explanatory annotations during the active learning process can enhance the performance of the text classifier. To simulate an ideal situation, it is assumed that in each iteration, the annotators have a complete understanding of the text classification, so the provided labels will \textbf{accurately correspond} to the source data. Furthermore, in the evaluation stage of Chapter \ref{chap:evaluation}, to eliminate the potential impact of the added labels and text volume on model performance, we will compare two scenarios: one with active learning that only added labels and another with active learning that added both labels and explanations.\\

\subsection{Explanation Annotation}
This experiment designed three different explanation annotation strategies. The provided explanations will be stored in the `Explanation.txt' set displayed in Figure \ref{fig4}, available for the text concatenation task prior to subsequent pre-training. System operators can choose among these three methods based on workforce allocation or cost control to better support the model's learning process:

\paragraph{The first strategy} involves previewing the texts in each iteration in advance and simulating the possible added explanations, just as an annotator would. The \textbf{preset explanations} will be saved in a `txt' file. When the annotation phase is executed, explanations are automatically extracted from this file. Although this method can reduce workforce consumption, it lacks adaptability and is only suitable for the dataset related to this experiment.

\paragraph{The second strategy} allows \textbf{humans} to provide input as explanations directly. This approach requires users to analyse the features of the selected text and provide their explanations. In each iteration, the annotator will focus on ten prominent texts of a particular category to quickly analyse these texts. This method incorporates human judgment and applies to other emergency-related data, offering some flexibility.

\paragraph{The last strategy} uses \textbf{OpenAI's model} to automatically provide explanations, reducing the need for human effort. This experiment chose the \textbf{`text-davinci-003'} engine for text analysis \cite{zhao2023chat}. Through the developer platform's \textbf{Playground} on the OpenAI interface, the prompt to be input into the model can be simulated in the Playground. The prompt uses \emph{"generate a string that only contains the words in the text that are only semantically equal to"} to connect the active learning strategy-selected prominent texts of a particular category. On the right navigation bar of the Playground, one can select the `Model', `Temperature', and maximum response length. The best-performing hyperparameters are chosen based on the reasonableness of the response. As shown in Table \ref{tab1}, for the classification ``deaths and injured", tests found that the `text-davinci-003" responses were far more reasonable and accurate than those of "text-ada-001". Other configurations tended to generate irrelevant information like geographical locations and numbers.\\

\begin{table}[h]
\centering
\begin{tabular}{cccl}
\hline
Model      & Temperature      & Top P      & Response      \\
\hline
text-ada-001      & $0.5     $ & $1     $ &``As many as \emph{98\%} of those killed ..  \emph{Pakistan Somalia}" \\
text-ada-001     & $1.0     $ & $1     $ & ``Hundreds dead in \emph{Pakistan}.. country \emph{6.0} quake, dozens hurt"\\
text-ada-001      & $1.0     $ & $0.5   $ & ``As many as \emph{98\%} of those killed strikes are civilians."\\
text-ada-001      & $1.5     $ & $1     $ & ``patrol western shuts the exchange \textit{peace 65 party}"\\
text-ada-003      & $0.5     $ & $1     $ & ``Hurt Death \emph{Pakistan} Quake Dozens Killed Loved Ones"\\
text-ada-003      & $1.0     $ & $1     $ & \textbf{``dozens hurt killed families loved ones RIP injured"}\\
text-ada-003      & $1.0     $ & $0.5   $ & ``Hurt, Dozens, Killed, \emph{Pakistan}, Families, Loved, Injured"\\
text-ada-003      & $1.5     $ & $1     $ & ``death Mortality over \emph{320 people} killed drone \emph{98\%} victims"\\
\hline
\end{tabular}
\caption{Results of the OpenAI model evaluation.}
\label{tab1}
\end{table}

After debugging, it was found that when the \textbf{`Temperature'} (the lower the temperature, the less randomness) is set to 1 and \textbf{`Top\_P'} (which controls diversity via nucleus sampling) is set to 1, the model's output is most reasonable. The Playground provides configuration code for the Python environment, allowing the code to be directly applied to the system. To successfully connect to the OpenAI interface, it is necessary to apply for an \textbf{OpenAI key} and load this key before generating responses.\\

It can be observed that using this model can analyse texts, selecting keywords that are semantically closest to the category label description to concatenate into a string. Ultimately, it returns a string that best reflects the characteristics of the text and helps the model learn from it as an added explanation. However, its drawback is that it's not friendly for the demand of generating multiple explanations at once or for active learning of hundreds of tokens. Although it reduces workforce costs, the generation of each token consumes certain funds.\\

% -----------------------------------------------------------------------------
\chapter{Implementation}
\noindent
\label{chap:Implementation}
This chapter primarily discusses the setup required to conduct the experiment. Section 4.1 introduces the fundamental environment in which the experiment is conducted, and Section 4.2 details the preparation and splitting of the dataset used in the experiment. Additionally, the performance metrics used in the evaluation in Chapter 5 and the rationale behind their selection are explained. Lastly, the stopping criteria adopted for the experiment (Section 4.3) and the operational modules (Section 4.4) are specified.\\

\section{Experimental Environment}
\label{section 4.1}
This experiment employs Python 3.9 as the primary development language for natural language processing tasks. Being highly suitable for natural language processing tasks, Python boasts a plethora of data science and machine learning libraries. This study utilised the capabilities provided by libraries such as PyTorch, Pandas, and Numpy for data analysis, pre-processing, and machine learning [Reference]. Python's high compatibility facilitates the connection to remote High Performance Computing Systems (HPC) within the context of this study using PyCharm, or running through Google Colab in conjunction with Google Drive. This research integrates PyTorch with Python to construct dynamic neural networks. By leveraging PyTorch's automatic gradient computation, the features of Dynamic Computational Graph, and the capability of CUDA for accelerated computation, a high level of flexibility and accuracy is maintained in the neural network model [Reference].\\

\section{Dataset}
\label{section 4.2}
The dataset used in this experiment simulates the text distribution of social media in emergencies. The dataset is prepared and split according to the framework described in section \ref{section 3.1} to meet the training requirements with a small amount of data in active learning.\\

\subsection{CrisisNLP}
In this experiment, the CrisisNLP dataset is utilised, which comprises data IDs, textual information, and descriptions of their corresponding categories. There are nearly 16,000 entries in total. This dataset requires preprocessing, splitting, pre-training, and classification. CrisisNLP is an open-source resource designed explicitly for humanitarian emergency and crisis response related natural language processing (NLP) research and development [ref]. The dataset contains interaction information from various large social media platforms, covering multiple crisis events, such as typhoons, earthquakes, and other events that require timely responses from relevant departments. The nature of the information can be divided into nine categories. The descriptions of these labels and their distribution are shown in the  Table \ref{tab2}.\\

\begin{table}[t]
\centering
\begin{tabular}{clc}
\hline
Label      & Description      & Percentage\\
\hline
0      & injured\_or\_dead\_people & $13\%     $\\
1      & missing\_trapped\_or\_found\_people & $2\%     $\\
2      & displaced\_people\_and\_evacuations & $3\%     $\\
3      & infrastructure\_and\_utilities\_damage & $8\%     $\\
4      & donation\_needs\_or\_offers\_or\_volunteering\_services & $14\%     $\\
5      & caution\_and\_advice & $6\%     $\\
6      & sympathy\_and\_emotional\_support & $11\%     $\\
7      & other\_useful\_information & $30\%     $\\
8      & not\_related\_or\_irrelevant & $13\%     $\\
\hline
\end{tabular}
\caption{Data distribution in CrisisNLP.}
\label{tab2}
\end{table}
According to the distribution in Table \ref{tab2}, it is evident that the data proportion is \textbf{highly imbalanced}. Label 7 has the highest proportion, and the information it provides is also the most mixed, accurately reflecting the distribution of emergency information in real-life scenarios. This poses a challenge for text classification tasks. Therefore, this dataset is of great value to researchers, as it documents real-time reactions and behaviours of people in various crisis situations and their corresponding data distribution. Consequently, researchers can develop more effective systems tailored for genuine data imbalances, addressing these crises, such as automatically detecting crisis-related social media posts or predicting the development trend of a crisis through analysing social media data.\\

\subsection{Data Preparation}
\label{section 4.2.2}
\paragraph{Preprocessing:}
Before feeding into the pre-trained model, preprocessing the tweets in the dataset is a critical step. The goal is to minimise the noise in the text and enhance the model's learning efficiency. Firstly, internet slang is standardised, converting informal abbreviations or colloquialisms into their standard forms and splitting camel-case named words. Subsequently, emojis in tweets are replaced with their textual counterparts using the emoji library, enabling the model to understand these symbols' meanings better. Lastly, website links, usernames, and ``$\#$” tags in tweets are cleaned or replaced since these elements typically don't contribute much to the model's learning. Such preprocessing steps help in reducing noise in tweets, thereby enhancing the model's learning efficiency and performance.

\paragraph{Dataset Splitting:}
For active learning tasks, splitting the dataset into unannotated and annotated datasets is essential. At the same time, the annotated dataset will be further split into training, validation, and testing sets. Active learning requires adding or removing data in each iteration. However, to ensure that the model's performance on unseen data can be tested fairly in each active learning iteration, the number of test samples must \textbf{remain constant}. Thus, the test set, unannotated and annotated datasets must be split \textbf{outside of the active learning loop}. The \textbf{training and validation sets will expand} with each iteration as the dataset grows, but their relative proportions remain unchanged. Since the data corresponding to label 1 only account for about 2\% of the total dataset, to ensure that all data subsets contain data corresponding to label 1, we use the 'StratifiedShuffleSplit' mechanism from the scikit-learn library to maintain the original data's class distribution. Compared to random splitting, this method provides a more accurate assessment of the classifier's performance.
\begin{table}[h]
\centering
\begin{tabular}{lllc}
\hline
\textbf{Data} & \textbf{Count} & & \textbf{Percentage} \\
\hline
Annotated Data & Training dataset & 2890 & 18\% \\
% \cline{2-4}
 & Validation dataset & 331 & 2\% \\
% \cline{2-4}
 & Test dataset & 1606 & 10\% \\
\hline
Unannotated Data & \multicolumn{2}{l}{11241} & 70\% \\
\hline
\end{tabular}
\caption{Distribution of datasets}
\label{tab3}
\end{table}

To meet the criteria of active learning, which achieves high performance with a small amount of training data, the number of annotated data in the first iteration of active learning is not the total data. As shown in Table \ref{tab3} above, the total is \textbf{only 30\%} of the entire dataset, with a length of 4827. The size of the unannotated dataset is set to \textbf{11241}. \\

When training with the \textbf{full dataset} (with the training set having the highest proportion), the combined length of the training and validation datasets in the annotated data will be 12845. The number of the test set remains unchanged both in the full dataset and in the amount of data required for active learning, with a length of 1606. 4\% of the data is sampled for annotation in each cycle. In the subsequent evaluation chapter, it was found that after 9 iterations, the model performance approached the model's performance corresponding to the validation set under the full data. At this time, the length of the unannotated dataset remains 6741.\\

\section{Performance Metrics}
The setting of performance metrics can significantly influence the effectiveness of active learning projects. Proper evaluation criteria can measure and compare the impacts of different active learning strategies and model frameworks on emergency systems in human learning. This, in turn, determines the choices of models, active learning strategies, and parameters. To assess the performance of active learning, the data is first partitioned according to the standards in Section \ref{section 4.2.2}. Then, the training begins on the training set. After each pool-based active learning iteration round, the trained model is evaluated on the test and validation sets using the predefined performance metrics. After each iteration, learning curves are plotted based on the performance metrics.\\

To comprehensively assess the performance of the model, the experiment utilizes two performance metrics: \textbf{accuracy and F1 score}. Accuracy measures the proportion of instances that the model correctly predicts relative to the total number of instances, and it can be a biased evaluation criterion. In contrast, the F1 score is the harmonic mean of precision and recall. Compared to accuracy, it eliminates bias and is more commonly used in multi-class problems [reference].\\
\begin{equation}
    F1\_score = \frac{2 }{(1/Precision + 1/Recall)}
\end{equation}
Where, Precision is the proportion of positive instances correctly predicted by the model out of all instances predicted as positive, and Recall is the proportion of actual positive instances that are correctly predicted by the model as positive. When dealing with imbalanced datasets like CrisisNLP, the F1 score can provide a more comprehensive measure of performance.\\

In addition, the \textbf{learning curve} can also serve as a performance evaluation criterion. By observing the changes in the learning curve, we can gain a deeper understanding of the model's learning capacity and stability. Tensorboard is used to record the learning curve during the active learning iteration process. The learning curve depicts the relationship between the model's performance and the number of samples used for training (number of active learning iterations), evaluating whether increasing the number of annotated datasets can effectively enhance the model's performance. Moreover, the experiment also logs the performance of the neural network classifier for each epoch during each training process through Tensorboard, ensuring the model achieves the best performance without overfitting after each active learning iteration.\\

% -----------------------------------------------------------------------------

\chapter{Critical Evaluation}
\label{chap:evaluation}

{\bf A topic-specific chapter, roughly 30\% of the total page-count} 
\vspace{1cm} 

\noindent
This chapter is intended to evaluate what you did.  The content is highly 
topic-specific, but for many projects, will have flavours of the following:

\begin{enumerate}
\item functional  testing, including analysis and explanation of the failure 
      cases,
\item behavioural testing, often including analysis of any results that 
      draw some form of conclusion wrt. the aims and objectives,
      and
\item evaluation of options and decisions within the project and/or a
      comparison with alternatives.
\end{enumerate}

\noindent
This chapter often acts to differentiate project quality: even if the work
completed is of high technical quality, critical yet objective evaluation 
and comparison of the outcomes is crucial.  In essence, the reader wants to
learn something, so the worst examples amount to simple statements of fact 
(e.g., ``graph X shows the result is Y''); the best examples are analytical 
and exploratory (e.g., ``graph X shows the result is Y, which means Z; this 
contradicts [1], which may be because I use a different assumption'').  As 
such, both positive {\em and}\/ negative outcomes are valid {\em if} presented 
in a suitable manner.


% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

{\bf A compulsory chapter,  roughly 10\% of the total page-count}
\vspace{1cm} 

\noindent
The concluding chapter(s) of a dissertation are often underutilised because they're 
too often left too close to the deadline: it is essential to allocate enough time and 
attention to closing off the story, the narrative, of your thesis.

Again, there is no single correct way of closing a thesis. 

One good way of doing this is to have a single chapter consisting of three parts:

\begin{enumerate}
\item (Re)summarise the main contributions and achievements, in essence
      summing up the content.
\item Clearly state the current project status (e.g., ``X is working, Y 
      is not'') and evaluate what has been achieved with respect to the 
      initial aims and objectives (e.g., ``I completed aim X outlined 
      previously, the evidence for this is within Chapter Y'').  There 
      is no problem including aims which were not completed, but it is 
      important to evaluate and/or justify why this is the case.
\item Outline any open problems or future plans.  Rather than treat this
      only as an exercise in what you {\em could} have done given more 
      time, try to focus on any unexplored options or interesting outcomes
      (e.g., ``my experiment for X gave counter-intuitive results, this 
      could be because Y and would form an interesting area for further 
      study'' or ``users found feature Z of my software difficult to use,
      which is obvious in hindsight but not during at design stage; to 
      resolve this, I could clearly apply the technique of Bloggs {\em et al.}.
\end{enumerate}

Alternatively, you might want to divide this content into two chapters: a penultimate chapter with a title such as ``Further Work" and then a final chapter ``Conclusions". Again, there is no hard and fast rule, we trust you to make the right decision. 

And this, the final paragraph of this thesis template, is just a bunch of citations, added to show how to generate a BibTeX bibliography. Sources that have been randomly chosen to be cited here include:





% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a database) then imported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.

\backmatter

\bibliography{sample_bibtex.bib}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendices; these are 
% the same as chapters in a sense, but once signalled as being appendices via
% the associated macro, LaTeX manages them appropriately.

\appendix

\chapter{An Example Appendix}
\label{appx:example}

Content which is not central to, but may enhance the dissertation can be 
included in one or more appendices; examples include, but are not limited
to

\begin{itemize}
\item lengthy mathematical proofs, numerical or graphical results which 
      are summarised in the main body,
\item sample or example calculations, 
      and
\item results of user studies or questionnaires.
\end{itemize}

\noindent
Note that in line with most research conferences, the examiners are not
obliged to read such appendices.

% =============================================================================

\end{document}
